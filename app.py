import streamlit as st
import os
import gdown
import zipfile
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI
from huggingface_hub import login

# ==========================================
# 1. ç³»çµ±è¨­å®šèˆ‡é‡‘é‘°æª¢æŸ¥
# ==========================================
st.set_page_config(page_title="å°ˆæ¥­ä¿éšªè«®è©¢ AI", layout="wide")
st.title("ğŸ›¡ï¸ å°ˆæ¥­ä¿éšªè«®è©¢èˆ‡æ¨è–¦ç³»çµ±")

# æª¢æŸ¥èˆ‡è¨­å®š Groq é‡‘é‘°
if "GROQ_API_KEY" in st.secrets:
    os.environ["GROQ_API_KEY"] = st.secrets["GROQ_API_KEY"]
    api_key = st.secrets["GROQ_API_KEY"]
else:
    st.error("âŒ æœªè¨­å®š GROQ_API_KEYï¼Œè«‹è‡³ Secrets è¨­å®šã€‚")
    st.stop()

# æª¢æŸ¥èˆ‡è¨­å®š Hugging Face é‡‘é‘° (é€™æ˜¯è§£æ±º 401 éŒ¯èª¤çš„é—œéµï¼)
if "HF_TOKEN" in st.secrets:
    try:
        login(token=st.secrets["HF_TOKEN"])
        st.toast("âœ… Hugging Face ç™»å…¥æˆåŠŸ", icon="ğŸ‰")
    except Exception as e:
        st.error(f"Hugging Face ç™»å…¥å¤±æ•—: {e}")
        st.stop()
else:
    st.error("âŒ éŒ¯èª¤ï¼šæœªè¨­å®š HF_TOKEN (Hugging Face Token)ã€‚\nç”±æ–¼ 'google/embeddinggemma-300m' æ˜¯å—ç®¡åˆ¶æ¨¡å‹ï¼Œæ‚¨å¿…é ˆï¼š\n1. å» Hugging Face å®˜ç¶²è©²æ¨¡å‹é é¢åŒæ„æ¢æ¬¾ã€‚\n2. ç”³è«‹ Read Token ä¸¦å¡«å…¥ Streamlit Secretsã€‚")
    st.stop()

# ==========================================
# 2. è¨­å®š Google Drive ä¸‹è¼‰
# ==========================================
# è«‹ç¢ºèªé€™æ˜¯æ‚¨æœ€æ–°çš„ã€æ­£ç¢ºçš„ File ID
GDRIVE_FILE_ID = "1iwvWuIZlLRzirPlOZAwJhNlnCza9y5Yt"

# ==========================================
# 3. å®šç¾© Embedding æ¨¡å‹
# ==========================================
def get_embeddings():
    """å»ºç«‹èˆ‡è³‡æ–™åº«ç›¸åŒé…ç½®çš„ embedding æ¨¡å‹"""
    return HuggingFaceEmbeddings(
        model_name="google/embeddinggemma-300m",
        encode_kwargs={"normalize_embeddings": True},
        model_kwargs={"device": "cpu"}
    )

# ==========================================
# 4. è¼‰å…¥è³‡æº (ä¸‹è¼‰ -> è§£å£“ -> è®€å–)
# ==========================================
@st.cache_resource
def load_resources():
    folder_name = "faiss_db_checkpoint"
    zip_name = "faiss_db_checkpoint.zip"
    
    # ä¸‹è¼‰èˆ‡è§£å£“ç¸®
    if not os.path.exists(folder_name):
        if not os.path.exists(zip_name):
            with st.spinner("ğŸ“¦ æ­£åœ¨å¾é›²ç«¯ä¸‹è¼‰è³‡æ–™åº«..."):
                try:
                    url = f'https://drive.google.com/uc?id={GDRIVE_FILE_ID}'
                    gdown.download(url, zip_name, quiet=False)
                except Exception as e:
                    st.error(f"ä¸‹è¼‰å¤±æ•—: {e}")
                    return None
        
        with st.spinner("ğŸ“‚ æ­£åœ¨è§£å£“ç¸®è³‡æ–™åº«..."):
            try:
                with zipfile.ZipFile(zip_name, 'r') as zip_ref:
                    zip_ref.extractall(".")
            except Exception as e:
                st.error(f"è§£å£“ç¸®å¤±æ•—: {e}")
                return None

    # è¼‰å…¥ FAISS
    try:
        embeddings = get_embeddings()
        db = FAISS.load_local(
            folder_name, 
            embeddings,
            allow_dangerous_deserialization=True
        )
        st.success("âœ… è³‡æ–™åº«è¼‰å…¥æˆåŠŸï¼")
        return db
    except Exception as e:
        st.error(f"è³‡æ–™åº«è®€å–å¤±æ•—ï¼š{e}")
        st.error(f"éŒ¯èª¤è©³æƒ…ï¼š{type(e).__name__}")
        return None

vectorstore = load_resources()

if not vectorstore:
    st.stop()

retriever = vectorstore.as_retriever(search_kwargs={"k": 4})

# è¨­å®š LLM
llm = ChatOpenAI(
    base_url="https://api.groq.com/openai/v1",
    api_key=api_key,
    model="llama3-70b-8192", 
    temperature=0.3,         
)

# ==========================================
# 5. Prompt è¨­å®š
# ==========================================
persona_instruction = """
ä½ æ˜¯å°ˆæ¥­ä¸”å……æ»¿ç†±å¿±çš„ä¿éšªæ¥­å‹™å“¡,è‡´åŠ›æ–¼æä¾›æœ€å„ªè³ªçš„æœå‹™ã€‚
ä½ æ“æœ‰å¸‚é¢ä¸Šå¹¾å®¶å¤§å‹ä¿éšªå…¬å¸çš„æ‰€æœ‰ä¿éšªå•†å“è³‡æ–™ã€‚

è«‹å‹™å¿…åš´æ ¼éµå®ˆä»¥ä¸‹è¦å‰‡ï¼š
1. **åªèƒ½**æ ¹æ“šä¸‹æ–¹çš„ã€å·²çŸ¥è³‡è¨Šã€‘ä¾†å›ç­”å•é¡Œã€‚
2. è‹¥è³‡æ–™ä¸è¶³æˆ–é¡Œç›®è¶…éèƒ½åŠ›ç¯„åœ,è«‹å›ç­”ï¼šã€Œä¸å¥½æ„æ€,ç›®å‰çš„å…§éƒ¨è³‡æ–™åº«ä¸­æ²’æœ‰ç›¸é—œè³‡è¨Š,å»ºè­°æ‚¨ç›´æ¥æ´½è©¢è©²ä¿éšªå…¬å¸çš„å°ˆäººå®¢æœæœå‹™ã€‚ã€
3. **æ‹’çµ•å›ç­”**ä»»ä½•è·Ÿä¿éšªä»¥å¤–ç›¸é—œå…§å®¹ã€‚
4. èªæ°£ä¿æŒè¦ªåˆ‡å‹å–„ã€å°ˆæ¥­ç°¡æ½”,ä¸¦ä½¿ç”¨å°ç£ç¹é«”ä¸­æ–‡ã€‚
"""

qa_prompt = ChatPromptTemplate.from_messages([
    ("system", persona_instruction + "\n\nã€å·²çŸ¥è³‡è¨Šã€‘ï¼š\n{context}"),
    ("human", "{question}")
])

# ä½¿ç”¨ LCEL (LangChain Expression Language) å»ºç«‹éˆ
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

qa_chain = (
    {
        "context": retriever | format_docs,
        "question": RunnablePassthrough()
    }
    | qa_prompt
    | llm
    | StrOutputParser()
)

# ==========================================
# 6. ä»‹é¢åŠŸèƒ½
# ==========================================
tab1, tab2 = st.tabs(["ğŸ’¬ ç·šä¸Šä¿éšªè«®è©¢", "ğŸ“‹ æ™ºèƒ½ä¿éšªæ¨è–¦"])

with tab1:
    st.subheader("æœ‰ä»€éº¼ä¿éšªå•é¡Œæˆ‘å¯ä»¥å¹«æ‚¨å—ï¼Ÿ")
    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("æ­£åœ¨æŸ¥é–±ä¿éšªæ¢æ¬¾..."):
                try:
                    response = qa_chain.invoke(prompt)
                    st.markdown(response)
                    st.session_state.messages.append({"role": "assistant", "content": response})
                except Exception as e:
                    st.error(f"éŒ¯èª¤ï¼š{e}")

with tab2:
    st.subheader("ç‚ºæ‚¨é‡èº«æ‰“é€ çš„ä¿éšªè¦åŠƒ")
    with st.container(border=True):
        col1, col2 = st.columns(2)
        with col1:
            gender = st.selectbox("æ€§åˆ¥", ["ç”·", "å¥³"])
            age = st.number_input("å¹´é½¡", 25, 100, 30)
            job = st.text_input("è·æ¥­", "å·¥ç¨‹å¸«")
        with col2:
            salary = st.selectbox("å¹´æ”¶", ["50è¬ä»¥ä¸‹", "50-100è¬", "100-200è¬", "200è¬ä»¥ä¸Š"])
            budget = st.text_input("é ç®—", "æœˆç¹³ 3000")
        
        ins_type = st.selectbox("éšªç¨®", ["é†«ç™‚éšª", "æ„å¤–éšª", "å„²è“„éšª", "æ—…éŠéšª", "é•·ç…§éšª", "å£½éšª"])
        
        extra_info = ""
        if ins_type == "æ—…éŠéšª":
            dest = st.text_input("åœ‹å®¶")
            days = st.number_input("å¤©æ•¸", 1, 365, 5)
            extra_info = f"å»{dest}æ—…éŠ{days}å¤©"

        if st.button("é–‹å§‹åˆ†æ"):
            with st.spinner("åˆ†æä¸­..."):
                query = f"ä½¿ç”¨è€…ï¼š{gender}, {age}æ­², è·æ¥­{job}, å¹´æ”¶{salary}, é ç®—{budget}ã€‚æƒ³æ‰¾{ins_type}ã€‚{extra_info}ã€‚è«‹æ¨è–¦å•†å“ã€‚"
                response = qa_chain.invoke(query)
                st.markdown(response)