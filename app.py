import streamlit as st
import os
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain_openai import ChatOpenAI

# ==========================================
# 1. å®šç¾©å®¢è£½åŒ– Embedding é¡åˆ¥ (é—œéµä¿®å¾©ï¼)
# ==========================================
# é€™æ˜¯ç‚ºäº†é…åˆæ‚¨åœ¨ Colab å»ºç«‹è³‡æ–™åº«æ™‚ä½¿ç”¨çš„è¨­å®š
class EmbeddingGemmaEmbeddings(HuggingFaceEmbeddings):
    def __init__(self, **kwargs):
        super().__init__(
            model_name="google/embeddinggemma-300m",
            encode_kwargs={"normalize_embeddings": True},
            **kwargs
        )

    def embed_documents(self, texts):
        # ä¿®æ”¹é€™è£¡ï¼šæ”¹æˆé€šç”¨çš„æè¿°ï¼Œæˆ–è€…ç›´æ¥ç”¨ "none"
        # é€™æ¨£å°±ä¸æœƒèª¤å°æ¨¡å‹ä»¥ç‚ºæ‰€æœ‰è³‡æ–™éƒ½æ˜¯å—å±±çš„
        texts = [f"title: ä¿éšªå•†å“æ¢æ¬¾ | text: {t}" for t in texts]
        return super().embed_documents(texts)

    def embed_query(self, text):
        # é€™æ˜¯å•ç­”æ™‚æœ€é—œéµçš„ä¸€è¡Œï¼Œä¿æŒåŸæ¨£å³å¯
        return super().embed_query(f"task: search result | query: {text}")
# ==========================================
# 2. ç³»çµ±è¨­å®šèˆ‡åˆå§‹åŒ–
# ==========================================
st.set_page_config(page_title="å°ˆæ¥­ä¿éšªè«®è©¢ AI", layout="wide")
st.title("ğŸ›¡ï¸ å°ˆæ¥­ä¿éšªè«®è©¢èˆ‡æ¨è–¦ç³»çµ±")

# æª¢æŸ¥ API Key
if "GROQ_API_KEY" in st.secrets:
    os.environ["GROQ_API_KEY"] = st.secrets["GROQ_API_KEY"]
    api_key = st.secrets["GROQ_API_KEY"]
else:
    st.error("âŒ æœªè¨­å®š GROQ_API_KEYï¼Œè«‹è‡³ Streamlit Secrets é€²è¡Œè¨­å®šã€‚")
    st.stop()

# è¼‰å…¥è³‡æ–™åº« (ä½¿ç”¨å¿«å–)
@st.cache_resource
def load_resources():
    try:
        # ä½¿ç”¨å‰›å‰›å®šç¾©çš„ Gemma æ¨¡å‹è¼‰å…¥
        embeddings = EmbeddingGemmaEmbeddings()
        
        # è¼‰å…¥ FAISS è³‡æ–™åº«
        # æ³¨æ„ï¼šæ ¹æ“šæ‚¨çš„ GitHub æˆªåœ–ï¼Œè³‡æ–™å¤¾åç¨±æ˜¯ 'faiss_db_checkpoint'
        db = FAISS.load_local(
            "faiss_db_checkpoint", 
            embeddings,
            allow_dangerous_deserialization=True
        )
        return db
    except Exception as e:
        # æŠŠå…·é«”éŒ¯èª¤ print å‡ºä¾†ï¼Œæ–¹ä¾¿é™¤éŒ¯
        print(f"è©³ç´°éŒ¯èª¤è¨Šæ¯: {e}")
        return None

# åˆå§‹åŒ–è³‡æº
vectorstore = load_resources()

if not vectorstore:
    st.error("âš ï¸ è³‡æ–™åº«è¼‰å…¥å¤±æ•—ï¼è«‹ç¢ºèªï¼š\n1. GitHub ä¸Šæ˜¯å¦æœ‰ 'faiss_db_checkpoint' è³‡æ–™å¤¾ï¼Ÿ\n2. è©²è³‡æ–™å¤¾å…§æ˜¯å¦æœ‰ 'index.faiss' å’Œ 'index.pkl'ï¼Ÿ\n3. æ˜¯å¦ä½¿ç”¨äº† Git LFS ä¸Šå‚³å¤§æª”æ¡ˆï¼Ÿ")
    # é€™è£¡å¯ä»¥é¡¯ç¤ºæ›´å¤šé™¤éŒ¯è³‡è¨Š
    st.info("æç¤ºï¼šå¦‚æœé€™æ˜¯ç¬¬ä¸€æ¬¡éƒ¨ç½²ï¼Œè«‹ç¢ºä¿ index.faiss æª”æ¡ˆå¤§å°æ­£å¸¸ (ä¸æ˜¯ 1KB çš„æŒ‡é‡æª”)ã€‚")
    st.stop()

retriever = vectorstore.as_retriever(search_kwargs={"k": 4})

# è¨­å®š LLM (ä½¿ç”¨ Groq)
llm = ChatOpenAI(
    base_url="https://api.groq.com/openai/v1",
    api_key=api_key,
    model="llama3-70b-8192", 
    temperature=0.3,         
)

# ==========================================
# 3. å®šç¾© Prompt Templates
# ==========================================

persona_instruction = """
ä½ æ˜¯å°ˆæ¥­ä¸”å……æ»¿ç†±å¿±çš„ä¿éšªæ¥­å‹™å“¡ï¼Œè‡´åŠ›æ–¼æä¾›æœ€å„ªè³ªçš„æœå‹™ã€‚
ä½ æ“æœ‰å¸‚é¢ä¸Šå¹¾å®¶å¤§å‹ä¿éšªå…¬å¸çš„æ‰€æœ‰ä¿éšªå•†å“è³‡æ–™ã€‚

è«‹å‹™å¿…åš´æ ¼éµå®ˆä»¥ä¸‹è¦å‰‡ï¼š
1. **åªèƒ½**æ ¹æ“šä¸‹æ–¹çš„ã€å·²çŸ¥è³‡è¨Šã€‘ä¾†å›ç­”å•é¡Œã€‚
2. è‹¥è³‡æ–™ä¸è¶³æˆ–é¡Œç›®è¶…éèƒ½åŠ›ç¯„åœï¼ˆä¾‹å¦‚è³‡æ–™åº«æ²’æœ‰è©²å•†å“ï¼‰ï¼Œè«‹å›ç­”ï¼šã€Œä¸å¥½æ„æ€ï¼Œç›®å‰çš„å…§éƒ¨è³‡æ–™åº«ä¸­æ²’æœ‰ç›¸é—œè³‡è¨Šï¼Œå»ºè­°æ‚¨ç›´æ¥æ´½è©¢è©²ä¿éšªå…¬å¸çš„å°ˆäººå®¢æœæœå‹™ã€‚ã€
3. **æ‹’çµ•å›ç­”**ä»»ä½•è·Ÿä¿éšªä»¥å¤–ç›¸é—œå…§å®¹ï¼ˆä¾‹å¦‚ï¼šé£Ÿè­œã€ç¨‹å¼ç¢¼ã€æ—…éŠæ™¯é»ä»‹ç´¹ã€å·´æ–¯å…‹è›‹ç³•æ€éº¼åšç­‰ï¼‰ï¼Œè«‹ç¦®è²Œæ‹’çµ•ä¸¦å°‡è©±é¡Œå¼•å°å›ä¿éšªã€‚
4. èªæ°£ä¿æŒè¦ªåˆ‡å‹å–„ã€å°ˆæ¥­ç°¡æ½”ï¼Œä¸¦ä½¿ç”¨å°ç£ç¹é«”ä¸­æ–‡ã€‚
"""

qa_prompt = PromptTemplate(
    template=persona_instruction + """
    
    ã€å·²çŸ¥è³‡è¨Šã€‘ï¼š
    {context}
    
    ä½¿ç”¨è€…å•é¡Œï¼š{question}
    
    å°ˆæ¥­æ¥­å‹™å“¡å›è¦†ï¼š
    """,
    input_variables=["context", "question"]
)

# å»ºç«‹æª¢ç´¢å•ç­”éˆ
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff",
    chain_type_kwargs={"prompt": qa_prompt}
)

# ==========================================
# 4. ä»‹é¢åŠŸèƒ½å¯¦ä½œ
# ==========================================

tab1, tab2 = st.tabs(["ğŸ’¬ ç·šä¸Šä¿éšªè«®è©¢", "ğŸ“‹ æ™ºèƒ½ä¿éšªæ¨è–¦"])

# --- åŠŸèƒ½ä¸€ï¼šChatbot ---
with tab1:
    st.subheader("æœ‰ä»€éº¼ä¿éšªå•é¡Œæˆ‘å¯ä»¥å¹«æ‚¨å—ï¼Ÿ")
    
    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("æ­£åœ¨æŸ¥é–±ä¿éšªæ¢æ¬¾..."):
                try:
                    response = qa_chain.invoke({"query": prompt})
                    result = response["result"]
                    st.markdown(result)
                    st.session_state.messages.append({"role": "assistant", "content": result})
                except Exception as e:
                    st.error(f"ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦ï¼š{e}")

# --- åŠŸèƒ½äºŒï¼šä¿éšªæ¨è–¦ ---
with tab2:
    st.subheader("ç‚ºæ‚¨é‡èº«æ‰“é€ çš„ä¿éšªè¦åŠƒ")
    with st.container(border=True):
        col1, col2 = st.columns(2)
        with col1:
            gender = st.selectbox("æ€§åˆ¥", ["ç”·", "å¥³"])
            age = st.number_input("å¹´é½¡", min_value=0, max_value=100, value=30)
            job = st.text_input("è·æ¥­", "ä¸€èˆ¬å…§å‹¤")
        with col2:
            salary = st.selectbox("å¹´æ”¶å…¥ç¯„åœ", ["50è¬ä»¥ä¸‹", "50-100è¬", "100-200è¬", "200è¬ä»¥ä¸Š"])
            budget = st.text_input("é ç®— (æœˆç¹³/å¹´ç¹³)", "æœˆç¹³ 3000 å…ƒ")
        
        ins_type = st.selectbox(
            "æ‚¨æ„Ÿèˆˆè¶£çš„ä¿éšªé¡å‹", 
            ["é†«ç™‚éšª", "æ„å¤–éšª", "å„²è“„éšª/æŠ•è³‡å‹", "æ—…éŠå¹³å®‰éšª", "é•·ç…§éšª", "å£½éšª"]
        )
        
        travel_details = ""
        if ins_type == "æ—…éŠå¹³å®‰éšª":
            st.info("âœˆï¸ åµæ¸¬åˆ°æ—…éŠéœ€æ±‚ï¼Œè«‹è£œå……ç´°ç¯€ï¼š")
            c1, c2 = st.columns(2)
            with c1:
                dest = st.text_input("æ—…éŠåœ‹å®¶", "æ—¥æœ¬")
            with c2:
                days = st.number_input("æ—…éŠå¤©æ•¸", min_value=1, value=5)
            travel_details = f"ï¼Œæ—…éŠç›®çš„åœ°ç‚º{dest}ï¼Œé è¨ˆæ—…éŠ{days}å¤©"

        if st.button("ğŸš€ é–‹å§‹åˆ†æä¸¦æ¨è–¦", type="primary"):
            with st.spinner("æ­£åœ¨åˆ†ææ‚¨çš„éœ€æ±‚ä¸¦æ¯”å°è³‡æ–™åº«..."):
                user_profile_query = f"""
                ä½¿ç”¨è€…åŸºæœ¬è³‡æ–™ï¼š
                - æ€§åˆ¥ï¼š{gender}
                - å¹´é½¡ï¼š{age}
                - è·æ¥­ï¼š{job}
                - å¹´æ”¶å…¥ï¼š{salary}
                - é ç®—ï¼š{budget}
                - ä¸»è¦éœ€æ±‚ï¼š{ins_type}{travel_details}
                
                ä»»å‹™ï¼šè«‹æ¨è–¦é©åˆçš„ã€{ins_type}ã€‘å•†å“ä¸¦èªªæ˜åŸå› ã€‚
                """
                try:
                    response = qa_chain.invoke({"query": user_profile_query})
                    st.success("åˆ†æå®Œæˆï¼ä»¥ä¸‹æ˜¯çµ¦æ‚¨çš„å°ˆæ¥­å»ºè­°ï¼š")
                    st.markdown(response["result"])
                except Exception as e:
                    st.error(f"åˆ†æå¤±æ•—ï¼š{e}")